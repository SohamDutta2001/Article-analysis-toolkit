# -*- coding: utf-8 -*-
"""blackcoffer (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NRECv6SyQsDugVq9DSPQaaZssPH1Cc53
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import math

def dx(url): #scraping
    url1 = url
    page = requests.get(url1)
    p2 = ''  # Initialize p2 with an empty string
    
    if page.status_code == 200:
        soup = BeautifulSoup(page.content, 'html.parser')
        q = soup.findAll(attrs={'class':'td-post-content'})
        p2 = q[0].text
        a = p2.rindex(".")
        p2 = p2[0:a]
        p2 = p2.replace("\xa0\n","").replace("\n","").replace("\xa0","").replace('“'," ").replace("”"," ").replace("..",".")
        
        tl = soup.findAll(attrs={'class':'td-post-title'})
        for j in tl:
            title = j.find("h1", attrs={'class':'entry-title'}).text.strip()
            p2 = "Title " + title + p2
            
    return p2

df=pd.read_excel("/content/Input.xlsx")

z=[]
y=[]

for i in range(len(df['URL'])):
  z.append(dx(df.iloc[i,1]))
  y.append(dx(df.iloc[i,1]))
  print(i)

stop_words=[]

my_file1 = open("/content/StopWords_Names.txt", "r") #stop_words
data = my_file1.read()
data_into_list = data.replace('\n',' ').split(" ")
new_list = [x for x in data_into_list if x != '']

stop_words.extend(new_list)

my_file1 = open("/content/StopWords_Auditor.txt", "r")
data = my_file1.read()
data_into_list = data.replace('\n',' ').split(" ")
new_list = [x for x in data_into_list if x != '']
stop_words.extend(new_list)

encodings = ["utf-8", "latin-1", "utf-16"]  # Add more encodings if necessary

for encoding in encodings:
    try:
        with open("/content/StopWords_Currencies.txt", "r", encoding=encoding) as my_file1:
            data = my_file1.read()
        break
    except UnicodeDecodeError:
        continue
else:
    print("Unable to decode the file using the available encodings.")
    # Handle the error appropriately

data_into_list = data.replace('\n', ' ').split(" ")
new_list = [x for x in data_into_list if x != '']
stop_words.extend(new_list)

my_file1 = open("/content/StopWords_DatesandNumbers.txt", "r")
data = my_file1.read()
data_into_list = data.replace('\n',' ').split(" ")
new_list = [x for x in data_into_list if x != '']
stop_words.extend(new_list)

my_file1 = open("/content/StopWords_Generic.txt", "r")
data = my_file1.read()
data_into_list = data.replace('\n',' ').split(" ")
new_list = [x for x in data_into_list if x != '']
stop_words.extend(new_list)

my_file1 = open("/content/StopWords_GenericLong.txt", "r")
data = my_file1.read()
data_into_list = data.replace('\n',' ').split(" ")
new_list = [x for x in data_into_list if x != '']
stop_words.extend(new_list)

my_file1 = open("/content/StopWords_Geographic.txt", "r")
data = my_file1.read()
data_into_list = data.replace('\n',' ').split(" ")
new_list = [x for x in data_into_list if x != '']
stop_words.extend(new_list)

len(stop_words)

from nltk.tokenize import word_tokenize

import nltk

nltk.download('punkt')

clean=[]

for i in range(len(df['URL'])):#removing stop words
    ty=word_tokenize(z[i])
    for j in range(len(ty)):
        for k in range(len(stop_words)):
            if (ty[j]==stop_words[k] or ty[j]=='?' or ty[j]=='!' or ty[j]==',' or ty[j]=='.'):
                ty[j]='' 
    ty = [x for x in ty if x != '']  
    clean.append(ty)

for i in range(len(z)): #all words without punctuations
    ty=word_tokenize(z[i])
    for j in range(len(ty)):
        for k in range(len(stop_words)):
            if (ty[j]=='?' or ty[j]=='!' or ty[j]==',' or ty[j]=='.'):
                ty[j]='' 
    ty = [x for x in ty if x != '']  
    z[i]=ty

negative_words=[]

encodings = ["utf-8", "latin-1", "utf-16"]  # Add more encodings if necessary

for encoding in encodings:
    try:
        with open("/content/negative-words.txt", "r", encoding=encoding) as my_file2:
            data = my_file2.read()
        break
    except UnicodeDecodeError:
        continue
else:
    print("Unable to decode the file using the available encodings.")
    # Handle the error appropriately

data_into_list = data.replace('\n', ' ').split(" ")
new_list = [x for x in data_into_list if x != '']
negative_words.extend(new_list)

positive_words=[]

my_file3 = open("/content/positive-words.txt", "r") #positive words
data = my_file3.read()
data_into_list = data.replace('\n',' ').split(" ")
new_list = [x for x in data_into_list if x != '']
positive_words.extend(new_list)

pos=[]

for i in range(len(clean)): #positive score
    ps=0
    ty=clean[i]
    for j in range(len(ty)):
        for k in range(len(positive_words)):
            if (ty[j]==positive_words[k]):
                ps=ps+1 

    pos.append(ps)

nos=[]

for i in range(len(clean)): #negative score
    ns=0
    ty=clean[i]
    for j in range(len(ty)):
        for k in range(len(negative_words)):
            if (ty[j]==negative_words[k]):
                ns=ns+1 
 
    nos.append(ns)

Polarity_Score=[]
Subjectivity_Score=[]

for i in range(len(pos)): #polarity and subjectivity
    cal=(pos[i]-nos[i])/(pos[i]+nos[i]+0.000001)
    cal1=(pos[i] + nos[i])/ ((len(clean[i])) + 0.000001)

    Polarity_Score.append(cal)
    Subjectivity_Score.append(cal1)

sentence=[]

for i in range(len(df['URL'])): # no of sentences
    ty=word_tokenize(y[i])
    count=ty.count('.')
    
    sentence.append(count)

Average_Number_of_Words_Per_Sentence=[]

for i in range(len(z)):
    if(sentence[i]!=0):
        Average =len(z[i])/sentence[i] #Average_Number_of_Words_Per_Sentence
    else:
        Average=0.0
    Average_Number_of_Words_Per_Sentence.append(Average)

syllable=[]

for i in range(len(z)): #syllable
    count=0
    ty=z[i]
    for j in range(len(ty)):
        if(ty[j].endswith('es')==False or ty[j].endswith('ed')==False):
              for k in range(len(ty[j])):

                        if (ty[j][k]=='a' or ty[j][k]=='e' or ty[j][k]=='i' or ty[j][k]=='o' or ty[j][k]=='u'or ty[j][k]=='A'or ty[j][k]=='E'or ty[j][k]=='I'or ty[j][k]=='O'or ty[j][k]=='U'):
                            count=count+1

    syllable.append(count)

complex_word_count=[]

for i in range(len(z)): #complex words
    w=[]
    ty=z[i]
    if (len(ty)>0):
        for j in range(len(ty)):
            count=0
            if(ty[j].endswith('es')==False and ty[j].endswith('ed')==False):
                        for k in range(len(ty[j])):
                                if (ty[j][k]=='a' or ty[j][k]=='e' or ty[j][k]=='i' or ty[j][k]=='o' or ty[j][k]=='u'or ty[j][k]=='A'or ty[j][k]=='E'or ty[j][k]=='I'or ty[j][k]=='O'or ty[j][k]=='U'):
                                    count=count+1
                        if (count>2):
                                 w.append(ty[j])
        complex_word_count.append(len(w))
    else:
        complex_word_count.append(0)

Percentage_of_Complex_words=[]

for i in range(len(df['URL'])): #% of complex words
    if(len(z[i])!=0):
        cal=(complex_word_count[i]/len(z[i]))*100
    else:
        cal=0
    Percentage_of_Complex_words.append(cal)

Fog_Index=[]

for i in range(len(df['URL'])): #fog index
    cal=(0.4 * (Average_Number_of_Words_Per_Sentence[i] + Percentage_of_Complex_words[i]))
    Fog_Index.append(cal)

Syllable_Count_Per_Word=[]

for i in range(len(df['URL'])): #Syllable_Count_Per_Word
    if(len(z[i])!=0):
        cal=syllable[i]/len(z[i])
    else:
        cal=0
    Syllable_Count_Per_Word.append(cal)

Personal_Pronouns=[]

for i in range(len(z)): #personal pronouns
    w=[]
    ty=(z[i])
    for j in range(len(ty)):
            if(ty[j]!='US'):
                if (ty[j]=='I' or ty[j]=='we' or ty[j]=='my' or ty[j]=='ours' or ty[j]=='us'):
                            w.append(ty[j])
                        
    Personal_Pronouns.append(len(w))

Average_Word_Length=[]
Total_char=[]

for i in range(len(z)): #total characters in each web sites
  
    ty=(z[i])
    count=0
    for j in range(len(ty)):
            count=count+len(ty[j])
    Total_char.append(count)

for i in range(len(df['URL'])): #avg word length
    if(len(z[i])!=0):
        cal=Total_char[i]/len(z[i])
    else:
        cal=0
    Average_Word_Length.append(cal)

df1=pd.read_excel("/content/Output Data Structure.xlsx")

for i in range(len(df1['URL'])):
    df1.iloc[i,2]=pos[i]
    df1.iloc[i,3]=nos[i]
    df1.iloc[i,4]=round(Polarity_Score[i],2)
    df1.iloc[i,5]=round(Subjectivity_Score[i],2)
    df1.iloc[i,6]=math.floor(Average_Number_of_Words_Per_Sentence[i])
    df1.iloc[i,7]=round(Percentage_of_Complex_words[i],2)
    df1.iloc[i,8]=round(Fog_Index[i],2)
    df1.iloc[i,9]=math.floor(Average_Number_of_Words_Per_Sentence[i])
    df1.iloc[i,10]=complex_word_count[i]
    df1.iloc[i,11]=len(z[i])
    df1.iloc[i,12]=math.ceil(Syllable_Count_Per_Word[i])
    df1.iloc[i,13]=Personal_Pronouns[i]
    df1.iloc[i,14]=math.floor(Average_Word_Length[i])

df1

df1.to_excel("Submission.xlsx") #output





